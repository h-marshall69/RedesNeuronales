{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Función de activación ReLU\n","metadata":{}},{"cell_type":"code","source":"inputs = [0, 2, -1, 3.3, -2.7, 1.1, 2.2, -100]\noutput = []\nfor i in inputs:\n    if i > 0:\n        output.append(i)\n    else:\n        output.append(0)\nprint(output)","metadata":{"execution":{"iopub.status.busy":"2024-10-17T09:52:14.138618Z","iopub.execute_input":"2024-10-17T09:52:14.139045Z","iopub.status.idle":"2024-10-17T09:52:14.152524Z","shell.execute_reply.started":"2024-10-17T09:52:14.139005Z","shell.execute_reply":"2024-10-17T09:52:14.151529Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"[0, 2, 0, 3.3, 0, 1.1, 2.2, 0]\n","output_type":"stream"}]},{"cell_type":"code","source":"inputs = [0, 2, -1, 3.3, -2.7, 1.1, 2.2, -100]\noutput = []\nfor i in inputs:\n    output.append(max(0, i))\nprint(output)","metadata":{"execution":{"iopub.status.busy":"2024-10-17T09:52:14.154580Z","iopub.execute_input":"2024-10-17T09:52:14.154996Z","iopub.status.idle":"2024-10-17T09:52:14.164982Z","shell.execute_reply.started":"2024-10-17T09:52:14.154961Z","shell.execute_reply":"2024-10-17T09:52:14.163850Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"[0, 2, 0, 3.3, 0, 1.1, 2.2, 0]\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\ninputs = [0, 2, -1, 3.3, -2.7, 1.1, 2.2, -100]\noutput = np.maximum(0, inputs)\nprint(output)","metadata":{"execution":{"iopub.status.busy":"2024-10-17T09:52:14.166701Z","iopub.execute_input":"2024-10-17T09:52:14.167213Z","iopub.status.idle":"2024-10-17T09:52:14.177949Z","shell.execute_reply.started":"2024-10-17T09:52:14.167158Z","shell.execute_reply":"2024-10-17T09:52:14.176617Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"[0.  2.  0.  3.3 0.  1.1 2.2 0. ]\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install nnfs","metadata":{"execution":{"iopub.status.busy":"2024-10-17T09:52:14.180698Z","iopub.execute_input":"2024-10-17T09:52:14.181075Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Requirement already satisfied: nnfs in /opt/conda/lib/python3.10/site-packages (0.5.1)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from nnfs) (1.26.4)\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nimport nnfs\nfrom nnfs.datasets import spiral_data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nfrom nnfs.datasets import spiral_data\n\n# Capa densa\nclass Layer_Dense:\n    # Inicialización de la capa\n    def __init__(self, n_inputs, n_neurons):\n        # Inicializar pesos y sesgos\n        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n        self.biases = np.zeros((1, n_neurons))\n\n    # Paso hacia adelante\n    def forward(self, inputs):\n        # Calcular los valores de salida a partir de las entradas, pesos y sesgos\n        self.output = np.dot(inputs, self.weights) + self.biases\n\n# Activación ReLU\nclass Activation_ReLU:\n    # Paso hacia adelante\n    def forward(self, inputs):\n        # Calcular los valores de salida a partir de la entrada\n        self.output = np.maximum(0, inputs)\n\n# Crear conjunto de datos\nX, y = spiral_data(samples=100, classes=3)\n\n# Crear capa densa con 2 características de entrada y 3 valores de salida\ndense1 = Layer_Dense(2, 3)\n\n# Crear activación ReLU (para usar con la capa densa)\nactivation1 = Activation_ReLU()\n\n# Hacer un paso hacia adelante de nuestros datos de entrenamiento a través de la capa densa\ndense1.forward(X)\n\n# Paso hacia adelante a través de la función de activación\n# Toma la salida de la capa anterior\nactivation1.forward(dense1.output)\n\n# Veamos la salida de las primeras muestras\nprint(activation1.output[:5])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Función de Activación Softmax","metadata":{}},{"cell_type":"code","source":"# Valores de la salida de la capa descritos previamente\nlayer_outputs = [4.8, 1.21, 2.385]\n# e - constante matemática, usamos E aquí para coincidir con un estilo de codifi\n# donde las constantes están en mayúsculas\nE = 2.71828182846 # también puedes usar math.e\n# Para cada valor en un vector, calcular el valor exponencial\nexp_values = []\nfor output in layer_outputs:\n    exp_values.append(E ** output) # ** - operador de potencia en Python\nprint('Valores exponenciados:')\nprint(exp_values)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Ahora normalizamos los valores\nnorm_base = sum(exp_values) # Sumamos todos los valores\nnorm_values = []\nfor value in exp_values:\n    norm_values.append(value / norm_base)\nprint('Valores exponenciados normalizados:')\nprint(norm_values)\nprint('Suma de los valores normalizados:', sum(norm_values)) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\n\n# Valores de la salida anterior cuando describimos qué es una red neuronal\nlayer_outputs = [4.8, 1.21, 2.385]\n\n# Para cada valor en un vector, calcular el valor exponencial\nexp_values = np.exp(layer_outputs)\nprint('Valores exponenciados:')\nprint(exp_values)\n\n# Ahora normalizar los valores\nnorm_values = exp_values / np.sum(exp_values)\nprint('Valores exponenciados normalizados:')\nprint(norm_values)\n\nprint('Suma de los valores normalizados:', np.sum(norm_values))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Ejemplo de entrada para obtener las probabilidades\ninputs = np.array([[4.8, 1.21, 2.385],\n                   [1.0, 2.0, 3.0]])\n\n# Obtén las probabilidades no normalizadas\nexp_values = np.exp(inputs)\n\n# Normalízalas para cada muestra\nprobabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nlayer_outputs = np.array([[4.8, 1.21, 2.385],\n [8.9, -1.81, 0.2],\n [1.41, 1.051, 0.026]])\nprint('Suma sin especificar el eje')\nprint(np.sum(layer_outputs))\nprint('Esto será idéntico a lo anterior ya que el valor por defecto es None:')\nprint(np.sum(layer_outputs, axis=None))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Otra forma de pensarlo con una matriz == axis 0: columnas:')\nprint(np.sum(layer_outputs, axis=0))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Pero queremos sumar las filas en su lugar, así con Python puro:')\nfor i in layer_outputs:\n    print(sum(i))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Entonces podemos sumar a lo largo del eje 1, pero nota la forma actual:')\nprint(np.sum(layer_outputs, axis=1))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Suma en el eje 1, pero mantiene las mismas dimensiones que la entrada:')\nprint(np.sum(layer_outputs, axis=1, keepdims=True))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nprint(np.exp(1))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(np.exp(10))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(np.exp(100))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(np.exp(1000))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nprint(np.exp(-np.inf), np.exp(0))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Activación Softmax\nclass Activation_Softmax:\n    # Pase hacia adelante\n    def forward(self, inputs):\n        # Obtener probabilidades no normalizadas\n        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n        # Normalizarlas para cada muestra\n        probabilidades = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n        self.output = probabilidades\nsoftmax = Activation_Softmax()\nsoftmax.forward([[1, 2, 3]])\nprint(softmax.output)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"softmax.forward([[-2, -1, 0]]) # Restando 3 - el máximo de la lista\nprint(softmax.output)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"softmax.forward([[0.5, 1, 1.5]])\nprint(softmax.output)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Crear el conjunto de datos\nX, y = spiral_data(samples=100, classes=3)\n# Crear la capa densa con 2 características de entrada y 3 valores de salida\ndense1 = Layer_Dense(2, 3)\n# Crear la activación ReLU (para ser usada con la capa densa):\nactivation1 = Activation_ReLU()\n# Crear la segunda capa densa con 3 características de entrada (ya que tomamos l\n# de la capa anterior aquí) y 3 valores de salida\ndense2 = Layer_Dense(3, 3)\n# Crear la activación Softmax (para ser usada con la capa densa):\nactivation2 = Activation_Softmax()\n# Realizar un pase hacia adelante de nuestros datos de entrenamiento a través de\ndense1.forward(X)\n# Realizar un pase hacia adelante a través de la función de activación\n# Toma la salida de la primera capa densa aquí\nactivation1.forward(dense1.output)\n# Realizar un pase hacia adelante a través de la segunda capa densa\n# Toma las salidas de la función de activación de la primera capa como entradas\ndense2.forward(activation1.output)\n# Realizar un pase hacia adelante a través de la función de activación\n# Toma la salida de la segunda capa densa aquí\nactivation2.forward(dense2.output)\n# Veamos la salida de las primeras muestras:\nprint(activation2.output[:5])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Código completo","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport nnfs\nfrom nnfs.datasets import spiral_data\n\nnnfs.init()\n\n# Capa Densa\nclass Layer_Dense:\n    # Inicialización de la capa\n    def __init__(self, n_inputs, n_neurons):\n        # Inicializar pesos y sesgos \n        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n        self.biases = np.zeros((1, n_neurons))\n\n    # Paso hacia adelante\n    def forward(self, inputs):\n        # Calcular los valores de salida a partir de las entradas, pesos y sesgos\n        self.output = np.dot(inputs, self.weights) + self.biases\n\n# Activación ReLU\nclass Activation_ReLU:\n    # Paso hacia adelante\n    def forward(self, inputs):\n        # Calcular los valores de salida a partir de las entradas\n        self.output = np.maximum(0, inputs)\n\n# Activación Softmax\nclass Activation_Softmax:\n    # Paso hacia adelante\n    def forward(self, inputs):\n        # Obtener probabilidades no normalizadas\n        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n        # Normalizarlas para cada muestra\n        probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n        self.output = probabilities\n\n# Crear conjunto de datos\nX, y = spiral_data(samples=100, classes=3)\n\n# Crear capa densa con 2 características de entrada y 3 valores de salida\ndense1 = Layer_Dense(2, 3)\n\n# Crear activación ReLU (para usar con la capa densa)\nactivation1 = Activation_ReLU()\n\n# Crear segunda capa densa con 3 características de entrada (ya que tomamos las salidas de la primera capa)\ndense2 = Layer_Dense(3, 3)\n\n# Crear activación Softmax (para usar con la capa densa)\nactivation2 = Activation_Softmax()\n\n# Realizar un paso hacia adelante de nuestros datos de entrenamiento a través de la primera capa densa\ndense1.forward(X)\n\n# Realizar un paso hacia adelante a través de la función de activación\n# Toma la salida de la primera capa densa aquí\nactivation1.forward(dense1.output)\n\n# Realizar un paso hacia adelante a través de la segunda capa densa\n# Toma las salidas de la función de activación de la primera capa como entradas\ndense2.forward(activation1.output)\n\n# Realizar un paso hacia adelante a través de la función de activación\n# Toma la salida de la segunda capa densa aquí\nactivation2.forward(dense2.output)\n\n# Ver la salida de las primeras muestras\nprint(activation2.output[:5])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Ejemplo Función Sigmoide","metadata":{}},{"cell_type":"markdown","source":"## Sin utilizar librerías","metadata":{}},{"cell_type":"code","source":"def sigmoid(x):\n    return 1 / (1 + 2.71828 ** -x)\n\n# Ejemplo de uso\ninputs = [0, 1, 2, 3]\noutputs = [sigmoid(x) for x in inputs]\nprint(\"Salidas de la función sigmoide sin librerías:\", outputs)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Usando librerías","metadata":{}},{"cell_type":"code","source":"import numpy as np\n\ndef sigmoid_np(x):\n    return 1 / (1 + np.exp(-x))\n\n# Ejemplo de uso\ninputs = np.array([0, 1, 2, 3])\noutputs = sigmoid_np(inputs)\nprint(\"Salidas de la función sigmoide utilizando NumPy:\", outputs)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}